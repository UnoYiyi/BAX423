{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text Data\n",
    "\n",
    "Apply the following text representation techniques (and any variations, such as stopword removals in BoW) on the Movies Review dataset (​http://ai.stanford.edu/~amaas/data/sentiment/​). Consider experimenting with the following: n-grams, stopword removal, punctuation removal, lemmatization, etc.\n",
    "1. Bag of Words (BoW)\n",
    "2. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "3. Feature hashing\n",
    "4. Apply sentiment analysis (two-class text classification) machine learning\n",
    "algorithm(s) to compare performance of models across these text representation techniques. Apply algorithm(s) of your choice (use any Python library of your choice) and compare results (such as number of features, model performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "\n",
    "\"Sentiment analysis, sometimes also called opinion mining, is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing the polarity of documents. A popular task in sentiment analysis is the classification of documents based on the expressed opinions or emotions of the authors with regard to a particular topic. \" - \"Python Machine Learning\"\n",
    "\n",
    "Team X was hired to tackle another tough problem faced by an entertainment company (confidential) who is interested in understanding how movie reviews could impact future performance of the movies and box office sales.  Specifically, they want to know the overall sentiment of the movie reviews from their customer base.\n",
    "\n",
    "The client provided Team X with a dataset that consists of reviews for over 50,000 polar movies.  The reviews are either positive (six stars or above on IMDb) or negative (fewer than five stars on IMDb).\n",
    "\n",
    "Team X conducted extensive data cleaning and preparation and used various tools to extract meaningful information from these movie reviews.  Eventually they built a machine learning model to predict whether a certain reviewer liked or disliked a movie.\n",
    "\n",
    "In the following sections, Team X will download the dataset, preprocess it into a useable format for machine learning tools, and extract meaningful information from a subset of these movie reviews to build a machine learning model that can predict whether a certain reviewer liked or disliked a movie.\n",
    "\n",
    "Today, informed by the discussions and presentations at the recent Sentiment Analysis Symposium, let’s examine the business case for sentiment analysis in the movie industry.\n",
    "\n",
    "##### What is the purpose of the sentimen tanalysis on the movie reviews?\n",
    "\n",
    "1. Companies could use the result to forecast the box office sales.\n",
    "\n",
    "2. The movie company (for example, Disney or Netflix) could decide the overall sentiment of the review and similarly understand the aggregate picture, the voice of the market rather than just of individuals? \n",
    "\n",
    "3. Can you discover relationships between sentiments and the characteristics of the people who expressed them as well trends over time and how opinions propagate through social networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt  # data visualization\n",
    "import seaborn as sns # data visualization\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/unoyiyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/unoyiyi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/unoyiyi/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/unoyiyi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:02\n"
     ]
    }
   ],
   "source": [
    "# change the `basepath` to the directory of the\n",
    "# unzipped movie dataset\n",
    "\n",
    "#basepath = '/Users/Sebastian/Desktop/aclImdb/'\n",
    "basepath = '/Users/unoyiyi/Desktop/MSBASpring/423/Final Project/aclImdb/'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffling the dataframe\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optional: Saving the assembled data as CSV file:\n",
    "        \n",
    "df.to_csv('./movie_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I normally do not watch local mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Believe it or not, this was at one time the wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After some internet surfing, I found the \"Home...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  My family and I normally do not watch local mo...          1\n",
       "1  Believe it or not, this was at one time the wo...          0\n",
       "2  After some internet surfing, I found the \"Home...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning Text Data\n",
    "\n",
    "The first important step before we build our bag-of words or any other model is to clean the text data by stripping it of all unwanted characters\n",
    "\n",
    "What we did here is to remove\n",
    "\n",
    "- a. Lemmatization (normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma)\n",
    "- b. Text Proeprocessor \n",
    "    - punctuations, line breaks\n",
    "    - convert all character into lowercase\n",
    "    - remove all of the HTLML markup \n",
    "- c. Remove the stopwords (is, the, for, to...etc;Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to Star Cinema!! Way to go, Jericho and Claudine!!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##a. lemmatization\n",
    "def lemma(words):\n",
    "    words_tag = pos_tag(words,tagset='universal')\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    for i, word_tag in enumerate(words_tag):\n",
    "        if word_tag[1] == \"NOUN\":\n",
    "                words[i] = lemmatiser.lemmatize(words[i],pos='n')\n",
    "        if word_tag[1] == \"VERB\":\n",
    "                words[i] = lemmatiser.lemmatize(words[i],pos='v')\n",
    "        if word_tag[1] == \"ADV\":\n",
    "                words[i] = lemmatiser.lemmatize(words[i],pos='r')\n",
    "        if word_tag[1] == \"ADJ\":\n",
    "                words[i] = lemmatiser.lemmatize(words[i],pos='a')            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##b. text preprocessor (include stopword removeval and lemmatization)\n",
    "def preprocessor(text, punc_num = True, root = True, stopword = True):\n",
    "    text = text.lower()\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '') \n",
    "    text = re.sub('[0-9]','',text.lower())\n",
    "    if root:\n",
    "        words = word_tokenize(text)\n",
    "        words = lemma(words)\n",
    "        text = ' '.join(words)\n",
    "    \n",
    "    if stopword:\n",
    "        custom_stopwords = stopwords.words('english')\n",
    "        words = [word for word in text.split() if word not in custom_stopwords]\n",
    "        text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ons star cinema way go jericho claudine'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test : ) : ( : )'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to test how the preprocessor works - the sentence include lemmatization, stopwords, and punctuations\n",
    "preprocessor(\"</a>This :) is :( a 50 tests :-)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family normally watch local movie simple reaso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>believe one time bad movie ever see since time...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>internet surf find homefront series dvd ioffer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one unheralded great work animation though mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sixty anyone long hair hip distant attitude co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  family normally watch local movie simple reaso...          1\n",
       "1  believe one time bad movie ever see since time...          0\n",
       "2  internet surf find homefront series dvd ioffer...          0\n",
       "3  one unheralded great work animation though mak...          1\n",
       "4  sixty anyone long hair hip distant attitude co...          0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide dataset into train and test\n",
    "\n",
    "we want divide the dataset into train and test dataset for future machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review'].values\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use the train_test_split to randomly split dataset to test and train. \n",
    "## In future practice, we could try different combination of the split ratio.\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Vectorization\n",
    "\n",
    "#### 3.1. Bag of Words (BoW)\n",
    "\n",
    "bag-of-words, which allows us to represent text as numerical feature vectors. The idea behind the bag-of-words model is quite simple and can be summarized as follows:\n",
    "1. We create a vocabulary of unique tokens—for example, words—from the entire set of documents.\n",
    "2. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular document.\n",
    "Since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros, which is why we call them sparse. Do not worry if this sounds too abstract; in the following subsections, we will walk through the process of creating a simple bag- of-words model step-by-step.\n",
    "\n",
    "\n",
    "- \"python machine learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By calling the fit_transform method on CountVectorizer, \n",
    "## we just constructed the vocabulary of the bag-of-words \n",
    "## model and transformed the data into sparse feature vectors:\n",
    "\n",
    "docs = df[\"review\"]\n",
    "count = CountVectorizer().fit(docs)\n",
    "bag_train = count.transform(X_train)\n",
    "bag_test = count.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25001, 90420)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check wether the train and test has the same shape. We need to focus on the number of features.\n",
    "bag_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 90420)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created:\n",
    "\n",
    "Each index position in the feature vectors shown here corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the  rst feature at index position 0 resembles the count of the word and, which only occurs in the last document, and the word is at index position 1 (the 2nd feature in the document vectors) occurs in all three sentences. Those values in the feature vectors are also called the raw term frequencies: *tf (t,d)*—the number of times a term t occurs in a document *d*. \"\n",
    "\n",
    "- from Python Machine Learning Book Chap 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90420\n"
     ]
    }
   ],
   "source": [
    "number_of_features = len(feature_names)\n",
    "print(number_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "\"When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. \" - chap 8 \"Python Machine Learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf-idf is the product of the term frequency and the inverse document frequency:\n",
    " \n",
    "$$tf-idf(t,d)=tf (t,d)×idf(t,d)$$\n",
    " \n",
    "Here the tf(t, d) is the term frequency and the inverse document frequency idf(t, d) is: \n",
    "\n",
    "$$idf(t,d)=log\\frac{n_{d}}{1+df(d,t)}$$\n",
    " \n",
    "where  $n_{d}$  is the total number of documents, and df(d, t) is the number of documents d that contain the term t. \n",
    "\n",
    "Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use TFidfTransformer to take raw term frequencies from CountVectorizer\n",
    "## and transforms them into tf-idfs\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$\n",
    "\n",
    "The tf-idf equation that was implemented in scikit-learn is as follows:\n",
    "\n",
    "$$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$\n",
    "\n",
    "While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the `TfidfTransformer` normalizes the tf-idfs directly.\n",
    "\n",
    "By default (`norm='l2'`), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector *v* by its L2-norm:\n",
    "\n",
    "$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$\n",
    "\n",
    "To make sure that we understand how TfidfTransformer works, let us walk\n",
    "through an example and calculate the tf-idf of the word is in the 3rd document.\n",
    "\n",
    "The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows:\n",
    "\n",
    "$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$\n",
    "\n",
    "Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency:\n",
    "\n",
    "$$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "vect.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90420\n"
     ]
    }
   ],
   "source": [
    "print(len(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform the datasets after tfidf techniques to test and train datasets\n",
    "tfidf_test = vect.transform(X_test)\n",
    "tfidf_train = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 90420)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check the shape of the test and train datasets\n",
    "tfidf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25001, 90420)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Feature Hashing\n",
    "\n",
    "- BoW models need to maintain in-memory vocabulary for encoding documents, which may require more memory for large datasets and slow down processing\n",
    "- An alternative to this is to use feature hashing or hashing trick to use a hash function and to find the token string name to feature integer index mapping\n",
    "- The HashingVectorizer class (in scikit-learn) implements this approach that can be used to hash words, then tokenize and encode documents as needed\n",
    "- This strategy has a key advantage of using low memory for large datasets, but the downside is that there is no way to convert the encoding back to a word\n",
    "\n",
    "\"class slide session 5 - page 19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "## By calling the transform method on HashingVectorizer, \n",
    "## we just constructed the vocabulary of the feature hashing\n",
    "## model and transformed the data into sparse feature vectors:\n",
    "## and put them into train and test datasets\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "feature_hashing_test = vectorizer.transform(X_test)\n",
    "feature_hashing_train = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment Analysis\n",
    "#### 4.1. Using Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.88     12527\n",
      "          1       0.87      0.88      0.88     12473\n",
      "\n",
      "avg / total       0.88      0.88      0.88     25000\n",
      "\n",
      "Accuracy of logistic regression classifier on test set: 0.88\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(bag_train,y_train)\n",
    "y_pred = logreg.predict(bag_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(bag_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.72      0.72     12527\n",
      "          1       0.72      0.72      0.72     12473\n",
      "\n",
      "avg / total       0.72      0.72      0.72     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(bag_train,y_train)\n",
    "pred_test = dtree.predict(bag_test)\n",
    "print(classification_report(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Use TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.87      0.89     12527\n",
      "          1       0.88      0.90      0.89     12473\n",
      "\n",
      "avg / total       0.89      0.89      0.89     25000\n",
      "\n",
      "Accuracy of logistic regression classifier on test set: 0.89\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(tfidf_train,y_train)\n",
    "y_pred = logreg.predict(tfidf_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(tfidf_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.71      0.71     12527\n",
      "          1       0.71      0.71      0.71     12473\n",
      "\n",
      "avg / total       0.71      0.71      0.71     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(tfidf_train,y_train)\n",
    "pred_test = dtree.predict(tfidf_test)\n",
    "print(classification_report(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Feature hashing Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.61      0.61     12527\n",
      "          1       0.61      0.62      0.62     12473\n",
      "\n",
      "avg / total       0.61      0.61      0.61     25000\n",
      "\n",
      "Accuracy of logistic regression classifier on test set: 0.61\n"
     ]
    }
   ],
   "source": [
    "#logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(feature_hashing_train,y_train)\n",
    "y_pred = logreg.predict(feature_hashing_test)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(feature_hashing_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.54      0.54     12527\n",
      "          1       0.54      0.54      0.54     12473\n",
      "\n",
      "avg / total       0.54      0.54      0.54     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(feature_hashing_train,y_train)\n",
    "pred_test = dtree.predict(feature_hashing_test)\n",
    "print(classification_report(y_test,pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 GRID SEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:,'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 13.7min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 75.2min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 101.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x116dd3400>} \n",
      "CV Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Result Comparison for different Methods and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Representation Techniques  | F1-Score for Logistic Regression|F1-Score for Decision Tree| F1-Score for GRID SEARCHCV\n",
    "------------- | -------------| ------| ---\n",
    "Bag of Words  | 0.88| 0.72| NA\n",
    "TF-IDF  | 0.89| 0.71|0.893\n",
    "Feature Hashing|0.61|0.54| NA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Solution & Key Highlights\n",
    "\n",
    "Team X was able to apply sentiment analysis (two-class text classification) machine learning algorithms to compare performance of models across these text representation techniques. Team X was able to apply different algorithms for different machine learning models and compare the results of model performance. Based on the result, Team X chose the Grid SearchCV method for the TF-IDF vectorization method to do the sentiment analysis because it has the highest accuracy score - 0.893.\n",
    "\n",
    "Key highlight: Team X found out that the running time for feature hashing technique is the fastest and the running time for the TF-IDF technique is the slowest. In addition, the most accurate GridSearchCV logistic regression on TF-IDF took three hours to run. \n",
    "\n",
    "If the client wants to optimize running time, TF-IDF Grid SearchCV might not be the optimal choice. Since the GRIDSEARCHCV TFIDF takes hours to run and the logistic regression of TF-IDF only takes a few minutes, the client should choose TF-IDF Logistic Regression given that the two have similar accuracy scores - both around 0.89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Learnings\n",
    "\n",
    "There are three key learning in this part of the project\n",
    "\n",
    "1.  When creating the train and test dataset, Team X needed to apply the text representation techniques and vectorize the entire dataset, rather than vectorizing the train and test dataset separately, as that will create different features and would impact the logistics regression model.\n",
    "\n",
    "For example, when Team X was doing the logistic regression model with the bag of words vectorization method, they noticed that the logistic regression model wouldn't run because the number of features in the bag of words test dataset is different from the number of features in the bag of words train dataset. Then, Team X went back to the vectorization part, and realized that the text vectorization was done separately for the train and test dataset. In the end, Team X solved the problem by first vectorizing the full review dataset, and then transforming the features into the train and test dataset to ensure they have the same number of features for modeling.\n",
    "   \n",
    "2. Variations of the text representation techniques are crucial to the model performance and the accuracy of the features. In the exercise, Team X used several ways to make the dataset more accurate to process:\n",
    "   a. Lemmatization (normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma)\n",
    "   b. Text Preprocessor \n",
    "        - punctuations, line breaks\n",
    "        - convert all character into lowercase\n",
    "        - remove all of the HTML markup \n",
    "   c. Remove the stopwords (is, the, for, to...etc;Stop Words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return vast amount of unnecessary information.)\n",
    "\n",
    "3. It can take a long time to reopen the python file after the vectorization part. If the client wants to  speed up the process of reopening files, they should consider removing or not displaying the features.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
